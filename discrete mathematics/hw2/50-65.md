# Задания 50-65, весна 2020

### 51. Сравните количество информации в одном броске честной монеты и одном броске честной игральной кости.

$H(монета)=g(2)=\log_2 2=1$

$H(кость)=g(6)=\log_2 6$

### 52. Докажите, что для монеты энтропия максимальна в случае честной монетыДокажите, что для монеты энтропия максимальна в случае честной монеты

$$H(p, 1-p)=-(p\log_2 p+(1-p)\log_2 (1-p))=-\left(p\log_2 \frac{p}{1-p}+\log_2(1-p)\right)$$
$$f(p):=-\left(p\log_2 \frac{p}{1-p}+\log_2(1-p)\right)$$
$$f'(p)=-\left(\log_2\frac{p}{1-p}+p\frac{1}{\frac{p}{1-p}\ln 2}\frac{(1-p)+p}{(1-p)^2}+\frac{1}{(1-p)\ln 2}\cdot(-1)\right)=-\log_2\frac{p}{1-p}-\frac{1}{(1-p)\ln 2} +\frac{1}{(1-p)\ln 2}=-\log_2\frac{p}{1-p}$$
$$-\log_2\frac{p}{1-p} \vee 0$$
$$-\frac{p}{1-p} - 1\wedge 0$$
$$2p \wedge 1$$

Итого $f\uparrow$ при $p<0.5,\ f\downarrow$ при $p>0.5 \Rightarrow \argmax f = 0.5$

### 53. Докажите, что для $n$ исходов энтропия максимальна если они все равновероятны

$$H(p_1\ldots p_n)=-\sum\limits_{i=1}^n p_i\log_2 p_i\leq \log_2 n \text{ (нерк, по Йенсену)}$$
$$H\left(\frac{1}{n}\ldots \frac{1}{n}\right)=-\sum\limits_{i=1}^n\frac{1}{n}\log_2\frac{1}{n}=\log_2 n$$

### 54. Найдите энтропию для геометрического распределения с $p=1/2$ (счетное число исходов, $i$-й исход происходит с вероятностью $1/2^i$).

Эти два номера _(54 и 55)_ решены, считая что $i\geq 1$.

$$\sum\limits_{i=0}^\infty x^{-i}=\frac{1}{1-x^{-1}} \text{ (геом. прогрессия)}$$
$$\left(\sum\limits_{i=0}^\infty x^{-i}\right)'=\left(\frac{1}{1-x^{-1}}\right)'$$
$$-\sum\limits_{i=0}^\infty ix^{-i-1}=\frac{-1}{(x-1)^2}$$
$$\sum\limits_{i=1}^\infty ix^{-i-1}=\frac{1}{(x-1)^2}$$
$$x^{-1}\sum\limits_{i=1}^\infty ix^{-i}=\frac{1}{(x-1)^2}$$
$$\sum\limits_{i=1}^\infty ix^{-i}=\frac{x^{-1}}{(x-1)^2}$$
Подставим $x=2$
$$\sum\limits_{i=1}^\infty i2^{-i}=2$$
$$H\left(\frac{1}{2}, \frac{1}{4}\ldots\right)=-\sum\limits_{i=1}^\infty2^{-i}\log_2 2^{-i}=\sum\limits_{i=1}^\infty i2^{-i}=2$$

В случае, если $i$ может быть $=0$: $H' = H - 2^0\log_2 2^{-0}=H-0=H$.

### 55. Найдите энтропию для геометрического распределения с произвольным $p$ (счетное число исходов, $i$-й исход происходит с вероятностью $(1−p)p^i$).

$$H((1-p)p, (1-p)p^2, \ldots)=-\sum\limits_{i=1}^\infty (1-p)p^i \log_2 ((1-p)p^i)=(p-1)\sum\limits_{i=1}^\infty p^i(\log_2 (1-p) + \log_2 p^i)=$$
$$=(p-1)\sum\limits_{i=1}^\infty p^i(\log_2 (1-p) + i\log_2 p)=(p-1)\left(\log_2 (1-p)\sum\limits_{i=1}^\infty p^i+\log_2 p\sum\limits_{i=1}^\infty ip^i\right)=$$
$$=(p-1)\left(\log_2 (1-p)\left(\sum\limits_{i=0}^\infty p^i-1\right)+\log_2 (p) \frac{p}{(p-1)^2}\right)=$$
$$=(p-1)\left(\log_2 (1-p)\left(\frac{1}{1-p}-1\right)+\log_2 (p) \frac{p}{(p-1)^2}\right)=$$
$$=(p-1)\left(\log_2 (1-p)\left(\frac{p}{1-p}\right)+\log_2 (p) \frac{p}{(p-1)^2}\right)=$$
$$=p\left(-\log_2 (1-p)+\log_2 (p) \frac{1}{(p-1)}\right)=$$

Если $i=0$ разрешено:
$$H'=H-(1-p)p^0\log_2((1-p)p^0)=H-(1-p)\log_2(1-p)=-\log_2 (1-p)+\log_2 (p) \frac{p}{p-1}$$

### 56. Доказать $H(A|B)+H(B)=H(B|A)+H(A)$

$$H(A|B)=-\sum\limits_{i = 1}^m P(b_i) \sum\limits_{j = 1}^n P(a_j | b_i) \log P(a_j | b_i)=$$
$$=-\sum\limits_{i = 1}^m \sum\limits_{j = 1}^n P(a_j \cap b_i) \log P(a_j | b_i)=$$
$$=-\sum\limits_{i = 1}^m \sum\limits_{j = 1}^n P(a_j \cap b_i) \log \frac{P(a_j \cap b_i)}{{P(b_i)}}=$$
$$=-\sum\limits_{i = 1}^m \sum\limits_{j = 1}^n P(a_j \cap b_i) \log P(a_j \cap b_i)+\sum\limits_{i = 1}^m \sum\limits_{j = 1}^n P(a_j \cap b_i) \log P(b_i)=$$
$$=-\sum\limits_{i = 1}^m \sum\limits_{j = 1}^n P(a_j \cap b_i) \log P(a_j \cap b_i)+\sum\limits_{i = 1}^m \log P(b_i)\sum\limits_{j = 1}^n P(a_j \cap b_i) =$$
$$=-\sum\limits_{i = 1}^m \sum\limits_{j = 1}^n P(a_j \cap b_i) \log P(a_j \cap b_i)+\sum\limits_{i = 1}^m P(b_i) \log P(b_i) =$$
$$=-\sum\limits_{i = 1}^m \sum\limits_{j = 1}^n P(a_j \cap b_i) \log P(a_j \cap b_i)-H(B)$$
$$\text{Аналогично } H(B|A) = -\sum\limits_{i = 1}^m \sum\limits_{j = 1}^n P(a_j \cap b_i) \log P(a_j \cap b_i)-H(A)$$

### 57. Что можно сказать про $H(A|B)$ если $a_i$ и $b_j$ независимы для любых $i$ и $j$?

$$P(a_i | b_j) = \frac{P(a_i \cap b_j)}{P(b_j)}=P(a_i)$$
$$H(A|B)=-\sum\limits_{i = 1}^m P(b_i) \sum\limits_{j = 1}^n P(a_j | b_i) \log P(a_j | b_i)=-\sum\limits_{i = 1}^m P(b_i) \sum\limits_{j = 1}^n P(a_j) \log P(a_j)=H(A)\sum\limits_{i = 1}^m P(b_i)=H(A) \text{ в силу полноты}$$

### 58. $H(A|A)$

$$H(A|A)=-\sum\limits_{i = 1}^m P(a_i) \sum\limits_{j = 1}^n P(a_j | a_i) \log P(a_j | a_i)=-\sum\limits_{i = 1}^m P(a_i) \sum\limits_{j = 1}^n P(a_j | a_i) \log P(a_j | a_i) = 0 \text{ , если } P(a_j | a_i)=0$$

Это логично - мы не получаем информацию, узнав что произошло $A$, если произошло $A$

### 59. Зафиксируем любой язык программирования. Колмогоровской сложностью слова $x$ называется величина $K(x)$ - минимальная длина программы на зафиксированном языке программирования, которая на пустом входе выводит $x$. Обозначим длину слова $x$ как $|x|$. Докажите, что $K(x)≤|x|+c$ для некоторой константы $c$.

Технически не указано, что язык полный по Тьюрингу или кому-то ещё  $\Rightarrow K(x)$ может не иметь значения.

Исходное утверждение не верно для фиксированного языка. Рассмотрим язык, который из своей программы выводит каждый второй символ. Тогда $K(x)=2|x|$, при $|x|\to\infty \quad 2|x|>c$.

Предположим, что речь идёт про все языки, т.е. $K(x)$ - минимум из длин программ на всех языках программирования, которые выводят $x$. Тогда это очевидно верно, т.к. $\exists P: x\mapsto x$, у него длина программы $|x|$. Тогда $K(x)\leq P(x)=|x|$

### 60. Предложите семейство слов $x_1,x_2,…,x_n,…,$ где $|x_i|$ строго возрастает и выполнено $K(x_i)=o(|x_i|)$.

$$K(x_i)=o(|x_i|) \Leftrightarrow \frac{K(x_i)}{|x_i|}\to0$$

$x_i=aa\ldots a$

```c
print(i * "a")
```

Длина программы = $const+len(i)=const+\log_{10} i$

$$0\leq\frac{K(x_i)}{|x_i|}\leq\frac{const+\log_{10} i}{i}=\frac{const}{i}+\frac{\log_{10} i}{i}\to0$$
$$\frac{K(x_i)}{|x_i|} \text{ зажато } 0 \text{ и } \frac{const}{i}+\frac{\log_{10} i}{i}, \text{ оба} \to0 \Rightarrow \frac{K(x_i)}{|x_i|}\to0$$

### 61. Предложите семейство слов $x_1,x_2,…,x_n,…,$ где $|x_i|$ строго возрастает и выполнено $K(x_i)=o(\log_2|x_i|)$.

$x_1:=a, x_2:=aaaa, x_3:=aaaaaaaaa (9)$ и т.д. по квадрату. Тогда будем делать `print((i ** 2) * "a")`, получается $o(\log_2\log_2|x_i|)$

Звучит как неправда - строки с минимальной энтропией оптимально сжимаются в $O(\log n)$. Если языки не полные, то можно брать языки, которые выдают константу, тогда длина программы $0$.

### 62. Колмогоровская сложность и энтропия Шеннона. Для слова $x$, в котором $i$-й символ алфавита встречается $f_i$ раз обозначим как $H(x)$ величину, равную энтропии случайного источника с распределением $p_i=f_i/|x|$. Докажите, что $K(x)≤nH(x)+O(\log n)$.

Сделаем язык, программа в котором - строка и длина искомой строки, язык выводит арифметическое раскодирование этой строки. Тогда если в программе $nH(x)+\log_2 n$ бит, то выдается строка с энтропией $H(x)$ длины $n$. $K(x)\leq H(x) + O(\log n)$

### 63. Докажите, что для любого $c>0$ найдется слово, для которого $K(x)<cnH(x)$

Рассмотрим строку из $n/2$ нулей и $n/2$ единиц. Кодируем её за $\lambda + \log_2 n$.
$$H(x)=\frac{1}{2}\log_2 2 + \frac{1}{2}\log_2 2=1$$
$$K(x)\leq \lambda + \log_2 n \stackrel{?}{<} cn$$
$$\frac{\lambda + \log_2 n}{n} \stackrel{?}{<} c$$
$$0\leftarrow\frac{\lambda}{n}+\frac{\log_2 n}{n}<c$$
Это верно для достаточно больших $n$.

### 64. Симуляция равновероятного дискретного распределения непрерывным за $O(1)$

Дано $f\to[0,1]$, домножим на $n$ и округлим вниз. Если получилось $n$, то сделаем `--`. Это ничего не ломает, т.к. $[0,1]\simeq[0,1)$

### 65. Пусть теперь мы хотим просимулировать с помощью непрерывного равномерного распределения дискретное распределение с распределением вероятностей $[p_1,…,p_n]$. Как это сделать за $O(\log n)$? Разрешается провести предподготовку за $O(n)$.

Надо замапить $x\in[0,1]$ в $\{1\ldots n\}$. Наивное решение - поделить отрезок в нужных пропорциях и заифвать границы, это $O(\log n)$. Сделаем умнее - бинпоиск. $O(n)$ предпосчета уйдёт на создание подсчёт префиксной суммы по $p$.

### 66. То же самое, но за $O(1)$ с предпосчетом за $O(n)$

Возьмём наиболее вероятный исход. Отдадим от него "кусок" вероятности наименее вероятному исходу, при этом кусок такой, что оставшаяся вероятность наиболее вероятного исхода теперь $1/n$. Сделав такое $n/2$ раз, получим набор из $2n$ отрезков, где каждые два отрезка в сумме имеют $1/n$.
